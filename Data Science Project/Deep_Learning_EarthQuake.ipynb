{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning EarthQuake.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM48zT+TIPucRVjuRl5w9Dz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravind598/Data_Science_Proj/blob/master/Data%20Science%20Project/Deep_Learning_EarthQuake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVIJfgvAwRD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA9Hoy8EsqmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "d2a50696-57d6-4ed5-a9e1-df30d1b68b05"
      },
      "source": [
        "!pip install tensorflow==1.1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import keras \n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import Model\n",
        "\n",
        "from sklearn.metrics import f1_score \n",
        "from sklearn.model_selection import KFold\n",
        "!pip install tensorflow==1.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/e4/b2a8bcd1fa689489050386ec70c5c547e4a75d06f2cc2b55f45463cd092c/tensorflow-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.4MB)\n",
            "\u001b[K     |████████████████████████████████| 31.4MB 140kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.18.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (0.34.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.1) (46.0.0)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed tensorflow-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.1 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.18.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.1) (46.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_f9miE0s9WS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/aravind598/Github/aravind598/train_labels.csv?token=ANW7SFBKZIJTONPGC67MZMS6SCJR2'\n",
        "urll = 'https://raw.githubusercontent.com/aravind598/Github/aravind598/train_values.csv?token=ANW7SFBK2YMNTYBWW44PE7C6SCJSE'\n",
        "url2 = 'https://raw.githubusercontent.com/aravind598/Github/aravind598/test_values.csv?token=ANW7SFHJZIY2XRWW5QVSW4S6SCS5I'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsR-fwd_s1Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = pd.read_csv(urll)\n",
        "train_y = pd.read_csv(url)\n",
        "test_x  = pd.read_csv(url2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jran0FjNtERR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "geo1 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_1_id\"], test_x[\"geo_level_1_id\"]])))\n",
        "geo2 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_2_id\"], test_x[\"geo_level_2_id\"]])))\n",
        "geo3 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_3_id\"], test_x[\"geo_level_3_id\"]])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHnrcnI-tFoP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "9a678481-0b5d-42f1-c3ff-d66b91d89bae"
      },
      "source": [
        "def NET():\n",
        "    inp = Input((geo3.shape[1],))\n",
        "    i1 = Dense(16, name=\"intermediate\")(inp)\n",
        "    x2 = Dense(geo2.shape[1], activation='sigmoid')(i1)\n",
        "    x1 = Dense(geo1.shape[1], activation='sigmoid')(i1)\n",
        "\n",
        "    model = Model(inp, [x2,x1])\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = NET()\n",
        "model.fit(geo3, [geo2, geo1], batch_size=128, epochs=10, verbose=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "347469/347469 [==============================] - 87s 250us/step - loss: 0.2042 - dense_1_loss: 0.0531 - dense_2_loss: 0.1512 - dense_1_acc: 0.9989 - dense_2_acc: 0.9659\n",
            "Epoch 2/10\n",
            "347469/347469 [==============================] - 94s 270us/step - loss: 0.0510 - dense_1_loss: 0.0051 - dense_2_loss: 0.0459 - dense_1_acc: 0.9993 - dense_2_acc: 0.9869\n",
            "Epoch 3/10\n",
            "347469/347469 [==============================] - 88s 254us/step - loss: 0.0143 - dense_1_loss: 0.0041 - dense_2_loss: 0.0102 - dense_1_acc: 0.9993 - dense_2_acc: 0.9977\n",
            "Epoch 4/10\n",
            "347469/347469 [==============================] - 88s 252us/step - loss: 0.0066 - dense_1_loss: 0.0035 - dense_2_loss: 0.0031 - dense_1_acc: 0.9993 - dense_2_acc: 0.9993\n",
            "Epoch 5/10\n",
            "347469/347469 [==============================] - 88s 255us/step - loss: 0.0043 - dense_1_loss: 0.0030 - dense_2_loss: 0.0013 - dense_1_acc: 0.9994 - dense_2_acc: 0.9997\n",
            "Epoch 6/10\n",
            "347469/347469 [==============================] - 88s 253us/step - loss: 0.0030 - dense_1_loss: 0.0023 - dense_2_loss: 6.1159e-04 - dense_1_acc: 0.9995 - dense_2_acc: 0.9999\n",
            "Epoch 7/10\n",
            "347469/347469 [==============================] - 89s 256us/step - loss: 0.0019 - dense_1_loss: 0.0016 - dense_2_loss: 3.2467e-04 - dense_1_acc: 0.9996 - dense_2_acc: 0.9999\n",
            "Epoch 8/10\n",
            "347469/347469 [==============================] - 90s 258us/step - loss: 0.0012 - dense_1_loss: 0.0011 - dense_2_loss: 1.7352e-04 - dense_1_acc: 0.9997 - dense_2_acc: 0.9999\n",
            "Epoch 9/10\n",
            "347469/347469 [==============================] - 94s 271us/step - loss: 8.0967e-04 - dense_1_loss: 7.1976e-04 - dense_2_loss: 8.9909e-05 - dense_1_acc: 0.9998 - dense_2_acc: 1.0000\n",
            "Epoch 10/10\n",
            "347469/347469 [==============================] - 87s 251us/step - loss: 5.4909e-04 - dense_1_loss: 5.0111e-04 - dense_2_loss: 4.7985e-05 - dense_1_acc: 0.9999 - dense_2_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f96ab427a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQNj75Clu_bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"Extract Intermediate Layer\" Function\n",
        "from keras import backend as K\n",
        "\n",
        "get_int_layer_output = K.function([model.layers[0].input],\n",
        "                                  [model.layers[1].output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrbrI-e_vWDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract GEO-Embeds for all train data points.\n",
        "# Then assign with train_data\n",
        "\n",
        "out = []\n",
        "for dat in geo3[:260601]:\n",
        "    layer_output = get_int_layer_output([[dat]])[0]\n",
        "    out.append(layer_output)\n",
        "\n",
        "out = np.array(out)\n",
        "out = np.squeeze(out)\n",
        "\n",
        "train_data = pd.get_dummies(train_x.copy())\n",
        "train_data = train_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)\n",
        "train_data = train_data.assign(geo_feat1=out[:,0],\n",
        "                               geo_feat2=out[:,1],\n",
        "                               geo_feat3=out[:,2],  \n",
        "                               geo_feat4=out[:,3],\n",
        "                               geo_feat5=out[:,4],    \n",
        "                               geo_feat6=out[:,5],\n",
        "                               geo_feat7=out[:,6],\n",
        "                               geo_feat8=out[:,7],\n",
        "                               geo_feat9=out[:,8],\n",
        "                               geo_feat10=out[:,9],\n",
        "                               geo_feat11=out[:,10],\n",
        "                               geo_feat12=out[:,11],\n",
        "                               geo_feat13=out[:,12],\n",
        "                               geo_feat14=out[:,13],\n",
        "                               geo_feat15=out[:,14],           \n",
        "                               geo_feat16=out[:,15])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCdUVeB27_x_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def threshold_arr(array):\n",
        "    # Get major confidence-scored predicted value.\n",
        "    new_arr = []\n",
        "    for ix, val in enumerate(array):\n",
        "        loc = np.array(val).argmax(axis=0)\n",
        "        k = list(np.zeros((len(val))))\n",
        "        k[loc]=1\n",
        "        new_arr.append(k)\n",
        "        \n",
        "    return np.array(new_arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAJA4wCXvWGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "e8bce213-4e44-4ff1-a455-8bbcc3cc526f"
      },
      "source": [
        "y = np.array(train_y[\"damage_grade\"])-1\n",
        "\n",
        "df = train_data.drop([\"building_id\"], axis=1)\n",
        "x = np.array(df)\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for ix, (train_index, test_index) in enumerate(kf.split(x)):\n",
        "    lgb_params = {\n",
        "        \"objective\" : \"multiclass\",\n",
        "        \"num_class\":3,\n",
        "        \"metric\" : \"multi_error\",\n",
        "        \"boosting\": 'gbdt',\n",
        "        \"max_depth\" : -1,\n",
        "        \"num_leaves\" : 30,\n",
        "        \"learning_rate\" : 0.1,\n",
        "        \"feature_fraction\" : 0.5,\n",
        "        \"min_sum_hessian_in_leaf\" : 0.1,\n",
        "        \"max_bin\":8192,\n",
        "        \"verbosity\" : 1,\n",
        "        \"num_threads\":6,\n",
        "      \n",
        "    }\n",
        "\n",
        "    x_train, x_val, y_train, y_val= x[train_index], x[test_index], y[train_index], y[test_index]\n",
        "\n",
        "    train_data = lgb.Dataset(x_train, label=y_train)\n",
        "    val_data   = lgb.Dataset(x_val, label=y_val)\n",
        "\n",
        "    lgb_clf = lgb.train(lgb_params,\n",
        "                        train_data,\n",
        "                        20000,\n",
        "                        valid_sets = [val_data],\n",
        "                        early_stopping_rounds=3000,\n",
        "                        verbose_eval = 1000)\n",
        "\n",
        "    y_pred = lgb_clf.predict(x_val)\n",
        "    print(\"F1-MICRO SCORE: \", f1_score(np.array(pd.get_dummies(y_val)), threshold_arr(y_pred), average='micro'))\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 3000 rounds.\n",
            "[1000]\tvalid_0's multi_error: 0.251741\n",
            "[2000]\tvalid_0's multi_error: 0.250436\n",
            "[3000]\tvalid_0's multi_error: 0.250705\n",
            "[4000]\tvalid_0's multi_error: 0.252796\n",
            "[5000]\tvalid_0's multi_error: 0.254005\n",
            "Early stopping, best iteration is:\n",
            "[2327]\tvalid_0's multi_error: 0.249899\n",
            "F1-MICRO SCORE:  0.7501007271541221\n",
            "Training until validation scores don't improve for 3000 rounds.\n",
            "[1000]\tvalid_0's multi_error: 0.250403\n",
            "[2000]\tvalid_0's multi_error: 0.247141\n",
            "[3000]\tvalid_0's multi_error: 0.248465\n",
            "[4000]\tvalid_0's multi_error: 0.249731\n",
            "[5000]\tvalid_0's multi_error: 0.250748\n",
            "Early stopping, best iteration is:\n",
            "[2070]\tvalid_0's multi_error: 0.246815\n",
            "F1-MICRO SCORE:  0.7531849577897161\n",
            "Training until validation scores don't improve for 3000 rounds.\n",
            "[1000]\tvalid_0's multi_error: 0.252322\n",
            "[2000]\tvalid_0's multi_error: 0.249885\n",
            "[3000]\tvalid_0's multi_error: 0.249731\n",
            "[4000]\tvalid_0's multi_error: 0.25048\n",
            "[5000]\tvalid_0's multi_error: 0.251995\n",
            "Early stopping, best iteration is:\n",
            "[2265]\tvalid_0's multi_error: 0.249213\n",
            "F1-MICRO SCORE:  0.7507866462010744\n",
            "Training until validation scores don't improve for 3000 rounds.\n",
            "[1000]\tvalid_0's multi_error: 0.248177\n",
            "[2000]\tvalid_0's multi_error: 0.245069\n",
            "[3000]\tvalid_0's multi_error: 0.246374\n",
            "[4000]\tvalid_0's multi_error: 0.246604\n",
            "Early stopping, best iteration is:\n",
            "[1787]\tvalid_0's multi_error: 0.244186\n",
            "F1-MICRO SCORE:  0.7558135072908673\n",
            "Training until validation scores don't improve for 3000 rounds.\n",
            "[1000]\tvalid_0's multi_error: 0.247755\n",
            "[2000]\tvalid_0's multi_error: 0.247218\n",
            "[3000]\tvalid_0's multi_error: 0.247064\n",
            "[4000]\tvalid_0's multi_error: 0.248484\n",
            "[5000]\tvalid_0's multi_error: 0.249578\n",
            "Early stopping, best iteration is:\n",
            "[2447]\tvalid_0's multi_error: 0.245702\n",
            "F1-MICRO SCORE:  0.7542977743668458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF1Ai1qXvWJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90y1oQxDvWL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsjTaG5ovWOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQPxl02kvWQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekJAUWuPvWTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p2wqP9OvWWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "901lLM3fvWYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c-Zx2GmvWbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8rqpFj1vWdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUokDmthvWgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}